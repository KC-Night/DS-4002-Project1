---
title: "Yelp Reviews: Sentiment Analysis"
author: "Kaitlyn Chou (group leader), Jensen Harvey, and Emily Friedman"
date: "2026-01-30"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(tidytext)
library(stringr)
library(ggplot2)
library(janitor)
library(randomForest)
library(caret)
```

## 1. Load and Prepare Data

```{r}

yelp <- read_csv("Yelp Restaurant Reviews.csv")


yelp <- yelp %>%
  clean_names() %>%              
  rename(
    url = yelp_url,              
    text = review_text           
  )


yelp <- yelp %>%
  mutate(
    sentiment = ifelse(rating >= 4, "Positive", "Negative"),
    sentiment = factor(sentiment, levels = c("Negative", "Positive"))
  )


cat("Dataset dimensions:", dim(yelp), "\n")
cat("\nSentiment distribution:\n")
print(table(yelp$sentiment))
cat("\nProportions:\n")
print(prop.table(table(yelp$sentiment)))
```

## 2. Exploratory Data Analysis

### Distribution of Binary Sentiment

```{r}
ggplot(yelp, aes(x = sentiment, fill = sentiment)) +
  geom_bar() +
  scale_fill_manual(values = c("Negative" = "darkred", "Positive" = "darkgreen")) +
  labs(title = "Distribution of Binary Sentiment",
       x = "Sentiment",
       y = "Count") +
  theme_minimal()
```

### Review Length by Sentiment

```{r}
yelp <- yelp %>%
  mutate(review_length = nchar(text))

yelp %>%
  group_by(sentiment) %>%
  summarise(
    avg_length = mean(review_length),
    median_length = median(review_length)
  ) %>%
  ggplot(aes(x = sentiment, y = avg_length, fill = sentiment)) +
  geom_col() +
  scale_fill_manual(values = c("Negative" = "darkred", "Positive" = "darkgreen")) +
  labs(title = "Average Review Length by Sentiment",
       x = "Sentiment",
       y = "Average Review Length (Characters)") +
  theme_minimal()
```

## 3. Text Preprocessing and Feature Engineering

```{r}

preprocess_text <- function(text_vector) {
  text_clean <- text_vector %>%
    tolower() %>%
    str_replace_all("http\\S+|www\\S+", "") %>%      
    str_replace_all("\\S+@\\S+", "") %>%             
    str_replace_all("[[:punct:]]", " ") %>%          
    str_replace_all("[[:digit:]]+", "") %>%         
    str_replace_all("\\s+", " ") %>%                 
    str_trim()
  
  return(text_clean)
}


yelp <- yelp %>%
  mutate(
    text_clean = preprocess_text(text),
    review_id = row_number()
  )
```

### Create TF-IDF Features

```{r}

food_stopwords <- tibble(word = c(
  "ice", "cream", "chocolate", "vanilla", "cookie", "cookies",
  "cake", "cakes", "flavor", "flavors", "bakery", "donut", "donuts"
))


tfidf_data <- yelp %>%
  select(review_id, sentiment, text_clean) %>%
  unnest_tokens(word, text_clean) %>%
  anti_join(stop_words, by = "word") %>%
  anti_join(food_stopwords, by = "word") %>%
  count(review_id, word) %>%
  bind_tf_idf(word, review_id, n)


discriminative_words <- tfidf_data %>%
  left_join(yelp %>% select(review_id, sentiment), by = "review_id") %>%
  group_by(word, sentiment) %>%
  summarise(
    total_tfidf = sum(tf_idf),
    count = n(),
    .groups = "drop"
  ) %>%
  pivot_wider(names_from = sentiment, values_from = c(total_tfidf, count), values_fill = 0) %>%
  mutate(
    total_count = count_Negative + count_Positive,
   
    discriminative_score = abs(total_tfidf_Positive - total_tfidf_Negative)
  ) %>%
  filter(total_count >= 10) %>%
  arrange(desc(discriminative_score)) %>%
  slice_head(n = 150) %>%  
  select(word)


dtm_wide <- tfidf_data %>%
  semi_join(discriminative_words, by = "word") %>%
  select(review_id, word, tf_idf) %>%
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0)


features_final <- yelp %>%
  select(review_id, sentiment, review_length) %>%
  left_join(dtm_wide, by = "review_id")


features_final[is.na(features_final)] <- 0

cat("Features matrix dimensions:", dim(features_final), "\n")
cat("Number of discriminative features:", ncol(dtm_wide) - 1, "\n")
```

## 4. Train-Test Split

```{r}
set.seed(42)


train_index <- createDataPartition(features_final$sentiment, p = 0.8, list = FALSE)

train_data <- features_final[train_index, ]
test_data <- features_final[-train_index, ]


X_train <- train_data %>% select(-review_id, -sentiment)
y_train <- train_data$sentiment

X_test <- test_data %>% select(-review_id, -sentiment)
y_test <- test_data$sentiment

cat("Training set distribution:\n")
print(table(y_train))
cat("\nTest set distribution:\n")
print(table(y_test))
cat("\nNumber of features:", ncol(X_train), "\n")
```

## 5. Model Training

### Model 1: Logistic Regression

```{r}


class_weights <- 1 / table(y_train)
weights <- ifelse(y_train == "Negative", 
                  class_weights["Negative"], 
                  class_weights["Positive"])

weights <- weights / sum(weights) * length(weights)


logistic_model <- glm(
  sentiment ~ ., 
  data = train_data %>% select(-review_id),
  family = binomial(link = "logit"),
  weights = weights
)


logistic_prob <- predict(logistic_model, newdata = X_test, type = "response")
logistic_pred <- factor(
  ifelse(logistic_prob > 0.5, "Positive", "Negative"),
  levels = c("Negative", "Positive")
)


logistic_accuracy <- mean(logistic_pred == y_test)
cat("Logistic Regression Test Accuracy:", round(logistic_accuracy * 100, 2), "%\n")
```

### Model 2: Random Forest

```{r}


y_train <- factor(y_train, levels = c("Negative", "Positive"))
y_test <- factor(y_test, levels = c("Negative", "Positive"))


neg_count <- sum(y_train == "Negative")
pos_count <- sum(y_train == "Positive")


rf_model <- randomForest(
  x = X_train,
  y = y_train,
  ntree = 200,
  mtry = floor(sqrt(ncol(X_train))),
  sampsize = c("Negative" = neg_count, "Positive" = neg_count),  # Balance by sampling
  importance = TRUE
)


rf_pred <- predict(rf_model, newdata = X_test)
rf_pred <- factor(rf_pred, levels = c("Negative", "Positive"))


rf_accuracy <- mean(rf_pred == y_test)
cat("Random Forest Test Accuracy:", round(rf_accuracy * 100, 2), "%\n")
```

## 6. Model Evaluation

### Confusion Matrices

```{r}

cat("=== Logistic Regression Confusion Matrix ===\n")
cm_logistic <- confusionMatrix(logistic_pred, y_test, positive = "Positive")
print(cm_logistic$table)
cat("\n")
print(cm_logistic$overall)


cat("\n=== Random Forest Confusion Matrix ===\n")
cm_rf <- confusionMatrix(rf_pred, y_test, positive = "Positive")
print(cm_rf$table)
cat("\n")
print(cm_rf$overall)
```

### Performance Metrics Comparison

```{r}

metrics_comparison <- data.frame(
  Model = c("Logistic Regression", "Random Forest"),
  Accuracy = c(cm_logistic$overall['Accuracy'], cm_rf$overall['Accuracy']),
  Precision = c(cm_logistic$byClass['Precision'], cm_rf$byClass['Precision']),
  Recall = c(cm_logistic$byClass['Recall'], cm_rf$byClass['Recall']),
  F1 = c(cm_logistic$byClass['F1'], cm_rf$byClass['F1'])
)

print(metrics_comparison)


metrics_comparison %>%
  pivot_longer(cols = c(Accuracy, Precision, Recall, F1), 
               names_to = "Metric", 
               values_to = "Score") %>%
  ggplot(aes(x = Metric, y = Score, fill = Model)) +
  geom_col(position = "dodge") +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  labs(title = "Model Performance Comparison",
       y = "Score") +
  theme_minimal()
```

### Feature Importance

```{r}

importance_df <- as.data.frame(importance(rf_model))
importance_df$feature <- rownames(importance_df)

top_20_features <- importance_df %>%
  arrange(desc(MeanDecreaseGini)) %>%
  head(20)


ggplot(top_20_features, aes(x = reorder(feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "darkblue") +
  coord_flip() +
  labs(title = "Top 20 Most Important Features",
       x = "Feature",
       y = "Mean Decrease in Gini") +
  theme_minimal()
```

## 7. Cross-Validation

```{r}


train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)


cv_rf <- train(
  x = X_train,
  y = y_train,
  method = "rf",
  trControl = train_control,
  tuneGrid = data.frame(mtry = c(5, 10, 15)),
  ntree = 200,
  metric = "ROC"
)

print(cv_rf)


plot(cv_rf, main = "Cross-Validation Results")
```

## 8. Results Summary

**Classification Task:**

-   Binary (Negative vs. Positive)

    -   Negative: 1-3 star ratings

    -   Positive: 4-5 star ratings

**Model Performance:**

-   Logistic Regression:

    -   \- Accuracy: 74.49 %

    -   \- Precision: 92.68 %

    -   \- Recall: 72.64 %

    -   \- F1 Score: 81.44 %

-   Random Forest:

    -   \- Accuracy: 78.56 %

    -   \- Precision: 89.93 %

    -   \- Recall: 81.28 %

    -   \- F1 Score: 85.39 %

-   Cross-Validated Random Forest:

    -    - ROC AUC: 0.832

**Top 5 Most Important Features:**

1.  review_length

2.  delicious

3.  amazing

4.  love

5.  dry

```{r}
sessionInfo()
```
